{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b48142a1-5a61-4e6b-bcba-41049efb09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import llm_api_keys\n",
    "\n",
    "openai.organization = os.environ['OPENAI_ORGANIZATION']\n",
    "openai.api_key = os.environ['OPENAI_API_KEY'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405dbcaf-0993-42fe-8ea6-1206c8333f44",
   "metadata": {
    "tags": []
   },
   "source": [
    "A $prompt$ refers to the input (text) data that is passed as input to the underlying (language) $model$. The model then outputs a $completion$ of the input text. For now, Lang Chain only supports text prompts, but will eventually support more complex data types (combinations of text, video, audio, etc.).\n",
    "\n",
    "Below, we walk through a simple example where we pass a $prompt$ to a language $model$, and print the $completion$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "258b87f2-b96d-4444-a880-c4ce013f8624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A model is a language that can be used to represent knowledge and experiences in your speech. A model can be used to represent different aspects of your life such as what you know and can be used to improve your speech communication.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "model = OpenAI(model_name=\"text-ada-001\")\n",
    "\n",
    "prompt_value = \"Hello, language model!\"\n",
    "\n",
    "completion = model(prompt_value)\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9897f-e80d-4481-987c-85abaeafa347",
   "metadata": {},
   "source": [
    "<font size=\"5\">Completion Parameters (basic) </font>\n",
    "\n",
    "\n",
    "    model_name: str = \"text-davinci-003\"\n",
    "    \"\"\"Model name to use.\"\"\"\n",
    "    \n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "    \n",
    "    max_tokens: int = 256\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\n",
    "    \n",
    "    -1 returns as many tokens as possible given the prompt and\n",
    "    the models maximal context size.\"\"\"\n",
    "    \n",
    "    top_p: float = 1\n",
    "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
    "    \n",
    "    frequency_penalty: float = 0\n",
    "    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\n",
    "    \n",
    "    presence_penalty: float = 0\n",
    "    \"\"\"Penalizes repeated tokens.\"\"\"\n",
    "    \n",
    "    n: int = 1\n",
    "    \"\"\"How many completions to generate for each prompt.\"\"\"\n",
    "    \n",
    "    best_of: int = 1\n",
    "    \"\"\"Generates best_of completions server-side and returns the \"best\".\"\"\"\n",
    "    \n",
    "Note, Langchain has seperate classes for $Large \\ Language \\ Models$ vs $Chat \\ Models$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85cd1b-4225-4cee-8d8a-44e1afd4f1fa",
   "metadata": {},
   "source": [
    "<font size=\"5\">Temperature </font>\n",
    "\n",
    "Temperature increases the uniformity of token probabilities. A temperature of 0 means the model will be more deterministic (always choosing the token with the highest probability), while as temperature increases towards 1, there will be some randomness in which token is chosen next (The model will randomly select among the most likely tokens). \n",
    "\n",
    "If you think of the token probabilities as a histogram, with one very high peak, you can think of the temperature as \"melting\" the mass at that peak, spreading it out into the other tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b3c14f3-09cc-443a-b0a2-cbe700674fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature set high,\n",
      "Words flow like a river wild,\n",
      "Language model thrives.\n",
      "\n",
      "Temperature set high,\n",
      "Words flow like a river wild,\n",
      "Language model thrives.\n",
      "\n",
      "Temperature set high,\n",
      "Words flow like a river wild,\n",
      "Language model thrives.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completion_list=[]\n",
    "\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "prompt_value = \"Write a Haiku about the parameter temperature, in the context of Large Language Models.\"\n",
    "\n",
    "for i in range(3):\n",
    "    completion_list.append(model(prompt_value))\n",
    "    print(completion_list[i])\n",
    "    print(\"\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9aa1e0ac-315f-4731-b180-5a6159a6cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature fluctuates,\n",
      "Large language models adjust heat,\n",
      "Precise output sought.\n",
      "\n",
      "Numbers rise and fall,\n",
      "Parameter temperature,\n",
      "LLMs learn all.\n",
      "\n",
      "Temperature fluctuates,\n",
      "Model output sharp or diffuse,\n",
      "Creative power.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completion_list=[]\n",
    "\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)\n",
    "\n",
    "prompt_value = \"Write a Haiku about the parameter temperature, in the context of Large Language Models.\"\n",
    "\n",
    "for i in range(3):\n",
    "    completion_list.append(model(prompt_value))\n",
    "    print(completion_list[i])\n",
    "    print(\"\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcfd71d-9892-47a7-b0f0-10bf6fe4241f",
   "metadata": {},
   "source": [
    "<font size=\"5\">Top-p </font>\n",
    "\n",
    "Top-p selects tokens from among the tokens with the highest probabilities, such that the sum of the probabilities of selected tokens is less than equal to p.\n",
    "\n",
    "It seems to keep the model more 'on topic' than temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e96415a-1bae-449f-91f2-629dc89d5d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top_p controls\n",
      "The probability of words\n",
      "In language models\n",
      "\n",
      "Top_p controls\n",
      "The probability of words\n",
      "In language models\n",
      "\n",
      "Top_p controls\n",
      "The probability of words\n",
      "In language models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completion_list=[]\n",
    "\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo\", top_p=.01)\n",
    "\n",
    "prompt_value = \"Write a Haiku about the parameter top_p, in the context of Large Language Models.\"\n",
    "\n",
    "for i in range(3):\n",
    "    completion_list.append(model(prompt_value))\n",
    "    print(completion_list[i])\n",
    "    print(\"\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31ae5523-0929-4f71-8ecf-90f9983348d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top_p constrains choice,\n",
      "Impacting how models speak,\n",
      "Language sculptor's tool.\n",
      "\n",
      "top_p reigns supreme\n",
      "Language models bow with ease\n",
      "Precision achieved\n",
      "\n",
      "Top_p's threshold\n",
      "Selects probable words to use\n",
      "Language models thrive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completion_list=[]\n",
    "\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo\", top_p=1)\n",
    "\n",
    "prompt_value = \"Write a Haiku about the parameter top_p, in the context of Large Language Models.\"\n",
    "\n",
    "for i in range(3):\n",
    "    completion_list.append(model(prompt_value))\n",
    "    print(completion_list[i])\n",
    "    print(\"\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b496e-f848-4024-a7e7-fc479dceafdb",
   "metadata": {},
   "source": [
    "Now let's look at how it critiques my explanations. Interestingly, $gpt \\ 3.5 \\ turbo$ (chatGPT) does a better job of directly answering the question than $gpt \\ 4$.\n",
    "\n",
    "I've tried multiple values of temperature/top_p, and I haven't been able to get gpt-4 to answer the questions. I likely could by adusting the prompt, but chatGPT does what I need in one-shot. **The lesson here is that bigger isn't necessarily better, and a choosing the right model can be much more efficient  than tweeking a prompt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e232e9c8-840b-465e-a8f3-88b14f1dec2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'temperature' and 'top_p' parameters play important roles in controlling the randomness and focus of text completions generated by large language models. \n",
      "\n",
      "Temperature affects the distribution of token probabilities. A temperature of 0 makes the model deterministic, always choosing the token with the highest probability. As the temperature approaches 1, the model introduces randomness, selecting tokens from a wider range of possibilities. You can visualize this as \"melting\" a histogram with a high peak, spreading the mass to other tokens\n"
     ]
    }
   ],
   "source": [
    "model = OpenAI(model_name=\"gpt-4\", temperature=.25, max_tokens=100)\n",
    "\n",
    "prompt_value = \"Can you critique my description of the roles of the 'temperature' and 'top_p' parameters for text completions generated by large language models? Is anything incorrect or missing? How could I make it more concise?  My explanation: Temperature increases the uniformity of token probabilities. A temperature of 0 means the model will be more deterministic (always choosing the token with the highest probability), while as temperature increases towards 1, there will be some randomness in which token is chosen next (The model will randomly select among the most likely tokens). If you think of the token probabilities as a histogram, with one very high peak, you can think of the temperature as 'melting' the mass at that peak, spreading it out into the other tokens. Top-p selects tokens from among the tokens with the highest probabilities, such that the sum of the probabilities of selected tokens is less than equal to p. It seems to keep the model more 'on topic' than temperature.\"\n",
    "\n",
    "print(model(prompt_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ca256cb-b18f-4a9e-a450-0d2b843b06d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your explanation is accurate and well-written, but it could be made more concise by combining some of the sentences: \"Temperature increases the uniformity of token probabilities. A temperature of 0 means the model will be more deterministic, while higher temperatures introduce randomness. Top-p selects tokens from among the highest probabilities, limited by a threshold sum.\"\n"
     ]
    }
   ],
   "source": [
    "model = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=.9, max_tokens=1000)\n",
    "\n",
    "prompt_value = \"Can you critique my description of the roles of the 'temperature' and 'top_p' parameters for text completions generated by large language models? Is anything incorrect or missing? How could I make it more concise?  My explanation: Temperature increases the uniformity of token probabilities. A temperature of 0 means the model will be more deterministic (always choosing the token with the highest probability), while as temperature increases towards 1, there will be some randomness in which token is chosen next (The model will randomly select among the most likely tokens). If you think of the token probabilities as a histogram, with one very high peak, you can think of the temperature as 'melting' the mass at that peak, spreading it out into the other tokens. Top-p selects tokens from among the tokens with the highest probabilities, such that the sum of the probabilities of selected tokens is less than equal to p. It seems to keep the model more 'on topic' than temperature.\"\n",
    "\n",
    "print(model(prompt_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
