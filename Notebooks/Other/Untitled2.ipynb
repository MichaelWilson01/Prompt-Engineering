{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48142a1-5a61-4e6b-bcba-41049efb09f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import llm_api_keys\n",
    "\n",
    "openai.organization = os.environ['OPENAI_ORGANIZATION']\n",
    "openai.api_key = os.environ['OPENAI_API_KEY'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405dbcaf-0993-42fe-8ea6-1206c8333f44",
   "metadata": {
    "tags": []
   },
   "source": [
    "A $prompt$ refers to the input (text) data that is passed as input to the underlying (language) $model$. The model then outputs a $completion$ of the input text. For now, Lang Chain only supports text prompts, but will eventually support more complex data types (combinations of text, video, audio, etc.).\n",
    "\n",
    "Below, we walk through a simple example where we pass a $prompt$ to a language $model$, and print the $completion$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258b87f2-b96d-4444-a880-c4ce013f8624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A model is a language that can be defined in a list of words. The words in this list can be used to create a new language, where the first word is the beginning of the language, and the last word is the end of the language. The words can be words or expressions, but expressions are more natural and easier to learn.\n",
      "\n",
      "The first word is \"language\" and the last word is \"model\". The model is a language that can be defined in a list of words. The words in this list can be used to create a new language, where the first word is the beginning of the language, and the last word is the end of the language. The words can be words or expressions, but expressions are more natural and easier to learn.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "model = OpenAI(model_name=\"text-ada-001\")\n",
    "\n",
    "prompt_value = \"Hello, language model!\"\n",
    "\n",
    "completion = model(prompt_value)\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ad6b8b-d312-4f46-8b3e-9a31e2a9985b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000314"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_num_tokens(completion)/1000*0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9897f-e80d-4481-987c-85abaeafa347",
   "metadata": {},
   "source": [
    "<font size=\"5\">Completion Parameters </font>\n",
    "\n",
    "\n",
    "    model_name: str = \"text-davinci-003\"\n",
    "    \"\"\"Model name to use.\"\"\"\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "    max_tokens: int = 256\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\n",
    "    -1 returns as many tokens as possible given the prompt and\n",
    "    the models maximal context size.\"\"\"\n",
    "    top_p: float = 1\n",
    "    \"\"\"Total probability mass of tokens to consider at each step.\"\"\"\n",
    "    frequency_penalty: float = 0\n",
    "    \"\"\"Penalizes repeated tokens according to frequency.\"\"\"\n",
    "    presence_penalty: float = 0\n",
    "    \"\"\"Penalizes repeated tokens.\"\"\"\n",
    "    n: int = 1\n",
    "    \"\"\"How many completions to generate for each prompt.\"\"\"\n",
    "    best_of: int = 1\n",
    "    \"\"\"Generates best_of completions server-side and returns the \"best\".\"\"\"\n",
    "    \n",
    "Note, Langchain has seperate classes for $Large \\ Language \\ Models$ vs $Chat \\ Models$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85cd1b-4225-4cee-8d8a-44e1afd4f1fa",
   "metadata": {},
   "source": [
    "<font size=\"5\">Temperature </font>\n",
    "\n",
    "Temperature increases the uniformity of token probabilities. A temperature of 0 means the model will be more deterministic (always choosing the token with the highest probability), while as temperature increases towards 1, there will be some randomness in which token is chosen next (The model will randomly select among the most likely tokens). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b3c14f3-09cc-443a-b0a2-cbe700674fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature set high,\n",
      "Words flow like a river wild,\n",
      "Language model thrives.\n",
      "\n",
      "Temperature set high,\n",
      "Words flow like a river wild,\n",
      "Language model thrives.\n",
      "\n",
      "Temperature set high,\n",
      "Words flow like a river wild,\n",
      "Language model thrives.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completion_list=[]\n",
    "\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "prompt_value = \"Write a Haiku about the parameter temperature, in the context of Large Language Models.\"\n",
    "\n",
    "for i in range(3):\n",
    "    completion_list.append(model(prompt_value))\n",
    "    print(completion_list[i])\n",
    "    print(\"\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9aa1e0ac-315f-4731-b180-5a6159a6cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature fluctuates,\n",
      "Large language models adjust heat,\n",
      "Precise output sought.\n",
      "\n",
      "Numbers rise and fall,\n",
      "Parameter temperature,\n",
      "LLMs learn all.\n",
      "\n",
      "Temperature fluctuates,\n",
      "Model output sharp or diffuse,\n",
      "Creative power.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completion_list=[]\n",
    "\n",
    "model = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=1)\n",
    "\n",
    "prompt_value = \"Write a Haiku about the parameter temperature, in the context of Large Language Models.\"\n",
    "\n",
    "for i in range(3):\n",
    "    completion_list.append(model(prompt_value))\n",
    "    print(completion_list[i])\n",
    "    print(\"\")\n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
